# Stock Market Data Pipeline

A data engineering pipeline built with Apache Airflow, Apache Spark, MinIO, and PostgreSQL for automated stock market data processing and analysis.

## ğŸ—ï¸ Architecture Overview
The pipeline consists of 6 main tasks orchestrated by Apache Airflow:

1. **Task 1**: Check API Availability
2. **Task 2**: Get Raw Data from Yahoo Finance
3. **Task 3**: Store Data in MinIO Object Storage
4. **Task 4**: Process Data with Apache Spark
5. **Task 5**: Retrieve Processed Data
6. **Task 6**: Load Data to PostgreSQL Warehouse


![Pipeline Architecture](./drawio/assests/Project.drawio.png)




## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Apache Airflow | Workflow management and scheduling |
| **Data Processing** | Apache Spark | Distributed data transformation |
| **Object Storage** | MinIO | S3-compatible data lake storage |
| **Data Warehouse** | PostgreSQL | Structured data storage |
| **Containerization** | Docker & Docker Compose | Application packaging and deployment |
| **Data Source** | Yahoo Finance API | Stock market data provider |

## ğŸ“ Project Structure

```
airflow-etl/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ stock_market.py              # Main Airflow DAG
â”œâ”€â”€ drawio                           # Drawio files
â”œâ”€â”€ include/
â”‚   â””â”€â”€ stock_market/
â”‚       â””â”€â”€ tasks.py                 # Custom task functions
â”œâ”€â”€ spark-apps/
â”‚   â””â”€â”€ format_stock_data.py         # Spark data processing script
â”œâ”€â”€ docker-compose.override.yml      # Multi-container orchestration
â”œâ”€â”€ Dockerfile                       # Custom Airflow image from Astro
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # Project documentation
```
